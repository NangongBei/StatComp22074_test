---
title: "Introduction to StatComp22074"
author: "22074"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp22074}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp22074__ is a simple R package, which mainly realizes the solution of the linear system $Ax=b$ based on the **Algebraic Multigrid (AMG)** method. At the same time, it reproduces several iterative algorithms including **Gauss-Seidel** iteration method, **Jacobi** iteration method, **Preconditioned Conjugate Gradient (PCG)** method and so on. At the same time, it realizes the **Coarsening** algorithm based on projection, and realizes the comparison between algorithms.

## Algebraic Multigrid

Multigrid method is a kind of idea, it was originally used to solve the problem of high and low frequency error in **partial differential equations**, and then gradually developed algebraic multigrid method can be used to solve the problem of **large-scale sparse diagonally dominant linear system**. 

Its design idea is based on the coarse mesh is conducive to reducing the low frequency error component, fine mesh can be used to reduce the high frequency error component, so through the conversion of different thickness and fine mesh, thus achieving the effect of reducing the high and low frequency error at the same time.

The flow of the two-level multigrid model can be summarized as follows.

### Two-level method


1. Pre-smoothing: Smooth $A^hu^h = f^h$ on $\Omega_h$ with some steps of a simple iterative scheme. This procedure gives an approximation $v^h$. Compute the residual $r^h = f^h − A^hv^h$.
	
2. Restrict the residual to the coarse grid $\Omega_{2h}$ using the restriction operator $I_h^{2h}$ (weighted restriction for finite difference methods, canonical restriction for finite element methods).
	
3. Solve the coarse grid equation $A^{2h}e^{2h}= I^{2h}_h (r_h)$ on $\Omega_{2h}$.
	
4. Prolongate $e^{2h}$ to $\Omega_h$ using the prolongation operator $I^h_{2h}$.
	
5. Post smoothing: Update $v^h := v^h + I^h_{2h}(e^{2h})$ (and apply smoother to $A^hu^h = f^h$ with the initial guess $v^h$).

### Multigrid Method

But if we assume that the network has $l+1$levels, in which $l\geq 0$, the smallest grid layer with $h$as the gap, let $l =2^l$, then the flow of Multigrid Method can be summarized as follows.


1. Apply the smoother $\nu_1$ times to $A^hu^h = f^h$ with the initial guess $v^h$. The results is denoted by $v_h$.

2. Compute $f^{2h} = I_h^{2h}r^h = I_h^{2h}( f^h − A^hv^h)$.

$\qquad$ a. Apply the smoother $\nu_1$ times to $A^{2h}u^{2h} = f^{2h}$ with the initial guess $v^{2h}= 0$. Denote the result by $v^{2h}$.

$\qquad$ b. Compute $f^{4h} = I_{2h}^{4h}r^{2h} = I_{2h}^{4h}( f^{2h} − A^{2h}v^{2h})$.

$\qquad$ c. ……
    
$\qquad$ $\qquad$ 1° Solve $A^{Lh}u^{Lh} = f^{Lh}$.
    
$\qquad$ d. ……

$\qquad$ e. Correct $v^{2h} := v^{2h} + I^{2h}_{4h}v^{4h}$.

$\qquad$ f. Apply smoother $\nu_2$ times to $A^{2h}u^{2h} = f^{2h}$ with the initial guess $v^{2h}$.
  
3. Correct $v^h := v^h + I_{2h}^hv^{2h}$.

4. Apply smoother $\nu_2$ times to $A^{h}u^{h} = f^{h}$ with the initial guess $v^{h}$.

Therefore, the basic framework of multigrid method can be written. First of all, before the actual solution, the matrix change of each layer can be established.

```{r}
Multigrid_Setup <- function(A,layer=0,CoarsenType='Projection',InterpType='Projection',pri=FALSE){
  Ah <- list(A)
  Qh <- c()
  dimension_A <- dim(A)[1]
  nnz <- sum(A!=0) # count non-zero number
  dense <- c(nnz/(dimension_A^2)) # calculate dense of matrix
  q <- max(floor((1 - dense)*5),1) # Adjust coarsening degree according to dense
  i <- 1
  # Set up A
  while(q > 1){
    Q <- restriction(Ah[[i]],n=q,CoarsenType=CoarsenType)
    Qh <- c(Qh,list(Q))
    Ah <- c(Ah,list(Q%*%Ah[[i]]%*%t(Q)))
    dimension_A <- c(dimension_A,dim(Ah[[i+1]])[1])
    nnz <- c(nnz,sum(Ah[[i+1]]!=0))
    dense <- c(dense,nnz[i+1]/(dimension_A[i+1]^2))
    i <- i + 1
    if(i == layer){
      break
    }
    q <- max(floor((1 - dense[i])*10),1)
  }
  # Print specific details of each layer change
  if(pri){
    pridata <- data.frame(dim_of_A=dimension_A,nnz=nnz,dense=dense,layer=1:i-1)
    print(pridata)
  }
  return(list(Ah=Ah,Qh=Qh,nnz=nnz,dense=dense,layer=i))
}
```

Then, according to the grid changes, the corresponding iterative algorithm can be written.

```{r}
# One-cycle
Multigrid_iter <- function(Ah,b,Qh,x0=0,presmooth='Gauss-Seidel',v1=2,
                           postsmooth='Gauss-Seidel',v2=2,solver='exact'){
  fh <- list(b)
  vh <- c()
  layer <- length(Qh)
  # Coarsening
  for(i in 1:layer){
    x <- Smoother(Ah[[i]],fh[[i]],x0,N=v1,SmoothMethod=presmooth)
    vh <- c(vh,list(x$x))
    f <- Qh[[i]]%*%(fh[[i]] - Ah[[i]] %*% vh[[i]])
    fh <- c(fh,list(f))
    x0 <- if(length(x0)>1) array(Qh[[i]]%*%x0) else x0
  }
  # solver
  layer_vh <- Smoother(Ah[[layer+1]],fh[[layer+1]],SmoothMethod=solver)
  vh <- c(vh,list(layer_vh$x))
  vsh <- vh
  # interpolation
  for(i in layer:1){
    f <- vh[[i]] + array(t(Qh[[i]])%*%vsh[[i+1]])
    x <- Smoother(Ah[[i]],fh[[i]],f,N=v2,SmoothMethod=postsmooth)
    vsh[[i]] <- x$x
  }
  return(list(x=vsh[[1]],error=norm(Ah[[1]]%*%vsh[[1]]-b)/norm(b)))
}
```

Meanwhile, the overall solution process can be written as follows.

```{r}
Multigrid_Solve <- function(Ah,b,Qh,x0=0,presmooth='Gauss-Seidel',v1=2,
                            postsmooth='Gauss-Seidel',v2=2,solver='exact',
                            tol=1e-5,max_iter=1e3,Precond=FALSE){
  error <- 1e3
  iter_time <- 0
  tag <- 1
  # precondition method
  if(Precond){
    # use PCG method
    result <- PCG_MG(Ah[[1]],b,x0,tol,max_iter,Ah,Qh,presmooth,v1,postsmooth,v2,solver)
    tag <- result$tag
    iter_time <- result$iter_time
    x0 <- result$x
    error <- result$error
  }
  # not precondition method
  else{
    while(error>tol){
      result <- Multigrid_iter(Ah,b,Qh,x0,presmooth,v1,postsmooth,v2,solver)
      x0 <- result$x
      error <- result$error
      iter_time <- iter_time + 1
      if((norm(matrix(result$x - x0))/norm(matrix(result$x))<tol) | (iter_time >= max_iter)){
        tag <- 0
        break
      }
    }
  }
  # show the result
  return(list(x=x0,iter_time=iter_time,error=error,tag=tag))
}
```

In addition, the implementation of Multigrid method needs to consider the following aspects.

1. Selection of smoother.

2. Selection of coarsening method.

3. Selection of interpolation algorithm.

4. Preconditioned or not.

## Smoother

The smoother of the Multigrid method mainly considers different linear solving iterative methods, and its iterative process can be shown as follows.

Purpose：Solve $Au = f$.

1. $A=M-N$, where $M$is a non-singular matrix,
$$Mu = Nu + f,$$
$$u = M^{-1}Nu + M^{-1}f := Su +  M^{-1}f.$$

2. Given the initial value $u^{(0)}$, perform the following iteration,
$$u^{(m+1)}= Su^{(m)} +  M^{-1}f,m=0,1,2,...,$$
Or do weighted iterations,
$$u^*= Su^{(m)} +  M^{-1}f,u^{(m+1)}=\omega u^*+(1-\omega)u^{(m)},$$
$$u^{(m+1)}= (\omega S+(1-\omega)I)u^{(m)} +  \omega M^{-1}f.$$
Definition of error $e^{(m)}=u-u^{(m)}$, residual $r^{(m)}=f-Au^{(m)}$, have the following conclusion,
	    
$$
\begin{aligned}
S e^{(m)} &=M^{-1} N u-S u^{(m)}=M^{-1} N u-u^{(m+1)}+M^{-1} f \\
&=M^{-1}(N u+f)-u^{(m+1)}=u-u^{(m+1)}=e^{(m+1)},
\end{aligned}
$$
$$Ae^{(m)} = Au-Au^{(m)} = f-Au^{(m)} = r^{(m)},$$
$$||e^{(m)}|| \leq ||S^m||||e^{(0)}||.$$
So if $\lim_{m \to \infty}||S^m||=0$, iterative convergence. The iterative method converges if and only if the $\rho(S) = \max_i|\lambda_i(S)|<1$.
	    
Meanwhile, Jacobi iteration takes $M=diag(A)$, and Gauss-Seidel iteration takes $M=D+L$, where $A=D+L+U$ is the diagonal, lower triangle and upper triangle part of $A$ respectively.

This R package considers two different smoothing methods, **Gauss-Seidel** iteration method and **Jacobi** iteration method, and makes the repetition as follows.

```{r}
Smoother <- function(Matrix_A, Matrix_b,x0=0,N=100,tol=-1,max_iter=1e3,SmoothMethod='Gauss-Seidel'){
  # Check whether the input meets the requirements
  if(!is.array(Matrix_A)){
    return(warning('A is not matrix.'))
  }
  else if(!is.array(Matrix_b)){
    return(warning('b is not array.'))
  }
  else if(N %% 1 != 0){
    return(warning('The number of iterations should be an integer.'))
  }
  else if(dim(Matrix_A)[1]!=dim(Matrix_b)[1]){
    return(warning('Dimension of A and b is not match.'))
  }
  else if(dim(Matrix_A)[1]!=dim(Matrix_A)[2]){
    return(warning('A is not a square matrix.'))
  }
  else if(length(x0) <= 1){
    x0 = array(rep(0,dim(Matrix_A)[2]))
  }
  else if(!is.array(x0)){
    return(warning('x0 is not array.'))
  }
  else if(dim(Matrix_A)[2]!=dim(x0)){
    return(warning('Dimension of A and x0 is not match.'))
  }
  tag <- 1 # tag whether converge
  m <- dim(Matrix_b)[1];n <- dim(x0)
  x_old <- x0 - 10
  x_new <- x0
  # Gauss-Seidel
  if(SmoothMethod == 'Gauss-Seidel'){
    if(tol<0){
      for(k in 1:N){
        x_old <- x_new
        x_new[1] = (Matrix_b[1,1] - sum(Matrix_A[1,2:n]*x_old[2:n]))/Matrix_A[1,1]
        for(i in 2:(n-1)){
          x_new[i] = (Matrix_b[i,1] - sum(Matrix_A[i,1:(i-1)]*x_new[1:(i-1)]) 
                      - sum(Matrix_A[i,(i+1):n]*x_old[(i+1):n]))/Matrix_A[i,i]
        }
        x_new[n] = (Matrix_b[n,1] - sum(Matrix_A[n,1:(n-1)]*x_new[1:(n-1)]))/Matrix_A[n,n]
      }
    }
    else{
      k <- 0
      while(norm(Matrix_A%*%x_new-Matrix_b)/norm(Matrix_b)>=tol){
        x_old <- x_new
        x_new[1] = (Matrix_b[1,1] - sum(Matrix_A[1,2:n]*x_old[2:n]))/Matrix_A[1,1]
        for(i in 2:(n-1)){
          x_new[i] = (Matrix_b[i,1] - sum(Matrix_A[i,1:(i-1)]*x_new[1:(i-1)]) 
                      - sum(Matrix_A[i,(i+1):n]*x_old[(i+1):n]))/Matrix_A[i,i]
        }
        x_new[n] = (Matrix_b[n,1] - sum(Matrix_A[n,1:(n-1)]*x_new[1:(n-1)]))/Matrix_A[n,n]
        k <- k+1
        if(norm(matrix(x_old - x_new))/norm(matrix(x_old))< tol){
          break
        }
        if(k>=max_iter){
          tag <- 0
          break
        }
      }
    }
  }
  # Jacobi 
  else if(SmoothMethod == 'Jacobi'){
    if(tol<0){
      for(k in 1:N){
        x_old <- x_new
        x_new = array((Matrix_b[,1] - Matrix_A %*% x_old + diag(diag(Matrix_A))%*%x_old)/diag(Matrix_A))
      }
    }
    else{
      k <- 0
      while(norm(Matrix_A%*%x_new-Matrix_b)/norm(Matrix_b)>=tol){
        x_old <- x_new
        x_new = array((Matrix_b[,1] - Matrix_A %*% x_old + diag(diag(Matrix_A))%*%x_old)/diag(Matrix_A))
        k <- k + 1
        if(norm(matrix(x_old - x_new))/norm(matrix(x_old))< tol){
          break
        }
        if(k>=max_iter){
          tag <- 0
          break
        }    
      }
    }
  }
  # exact solve
  else if(SmoothMethod == 'exact'){
    k = -1
    tag = -1
    x_new = array(solve(Matrix_A)%*%Matrix_b)
    x_old = x_new
  }
  else{
    return(warning('No this smoothing method.'))
  }
  return(list(x=x_new,iter_time=k,tag=tag,tol = norm(matrix(x_old - x_new))/norm(matrix(x_old)),
              error=norm(Matrix_A%*%x_new-Matrix_b)/norm(Matrix_b)))
}
```

## Coarsening and Interpolation Algorithm

**StatComp22074** considers the coarsening method based on projection, considering the idea of node aggregation, and takes the sparsity of the current matrix as the standard of the next coarsening degree. For the sake of simplicity, the interpolating matrix is defined as the transpose of the coarse matrix.

The projection matrix looks like the following.

$$Q_{i}^T=\left[\begin{array}{ccc}
    \frac{1}{\sqrt{n_{1}}} & & \\
    \vdots & & \\
    \frac{1}{\sqrt{n_{1}}} & & \\
    & \ddots & \\
    & & \frac{1}{\sqrt{n_{\alpha}}} \\
    & & \vdots\\
    & & \frac{1}{\sqrt{n_{\alpha}}}
    \end{array}\right] .$$
with

$$x_i=(x_1^1,...,x_{n_1}^1,x_1^2,...,x_{n_2}^2,...,x_1^\alpha,...,x_{n_\alpha}^\alpha)^T$$
	
$x_1^j,...,x_{n_j}^j$: Internal node of the $j$ partition.
	
$n_j$: Number of nodes in the $j$ partition.
	
$\alpha$: Number of partitions.
	
and the size of matrix will change from $n$ to $\alpha$ in the next level.

```{r}
restriction <- function(Matrix_A,n=2,CoarsenType='Projection'){
  dimC <- dim(Matrix_A)
  if(dimC[1]%%n==0){
    Q <- matrix(rep(0,dimC[1]/n*dimC[1]),dimC[1]/n,dimC[1])
    for(i in 1:(dimC[1]/n)){
      Q[i,(n*(i-1)+1):(n*i)] <- 1/sqrt(n)
    }
  }
  else{
    q <- dimC[1] %/% n + 1
    r <- dimC[1] %% n
    Q <- matrix(rep(0,q*dimC[1]),q,dimC[1])
    for(i in 1:(q-1)){
      Q[i,(n*(i-1)+1):(n*i)] <- 1/sqrt(n)
    }
    Q[q,(n*i+1):dimC[1]] <- 1/sqrt(r)
  }
  return(Q)
}
```

## Precondition

If precondition is not selected, then AMG will iterate over each cycle as the solver until convergence. If AMG is used as the preconditioner, StatComp22074 replicates the Preconditioned Conjugate Gradient method (PCG) and integrates it into AMG. The algorithm flow of PCG is as follows.

$r_0:=b-Ax_0$

$z_0:=M^{-1}r_0$

$p_0:=z_0$

$k:=0$

**repeat**

$\qquad$ $\alpha_k:=\frac{r_k^Tz_k}{p_k^TAp_k}$

$\qquad$ $x_{k+1}:=x_k+\alpha_k p_k$

$\qquad$ $r_{k+1}:=r_k-\alpha_k Ap_k$

$\qquad$ if $r_{k+1}$ is sufficiency small then exit loop else

$\qquad$ $z_{k+1}:=M^{-1}r_{k+1}$

$\qquad$ $\beta_k:=\frac{z_{k+1}^Tr_{k+1}}{z_{k}^Tr_{k}}$

$\qquad$ $p_{k+1}:=z_{k+1}+\beta_k p_k$

$\qquad$ $k:=k+1$

**end repeat**

with $z_{k}$ is the result of $r_k$ after an AMG cycle.

```{r}
PCG_MG <- function(A,b,x,tol=1e-5,max_iter=1e3,Ah,Qh,
                   presmooth='Gauss-Seidel',v1=2,
                   postsmooth='Gauss-Seidel',v2=2,solver='exact'){
  if(length(x) <= 1){
    x = array(rep(0,dim(A)[2]))
  }
  tag <- 1
  r_new = b - A %*% x
  # preconditioner
  z = Multigrid_iter(Ah,r_new,Qh,x0=x,presmooth,v1,postsmooth,v2,solver)
  z_new = z$x
  p = z_new
  k = 0
  while(1){
    r_old = r_new
    z_old = z_new
    alpha = t(r_old) %*% z_old / (p %*% A %*% p)
    x = x + alpha[1,1] * p
    r_new = r_old - alpha[1,1] * A %*% p
    error = norm(r_new)/norm(b)
    # end repeat
    if(error<tol){
      break
    }
    # preconditioner
    z = Multigrid_iter(Ah,r_new,Qh,x0=x,presmooth,v1,postsmooth,v2,solver)
    z_new = z$x
    beta = t(r_new) %*% z_new / t(r_old) %*% z_old
    p = z_old + beta[1,1] * p
    k = k + 1
    # exceed max iteration time
    if(sum((alpha[1,1] * p)^2)/sum(x^2)<tol | k >= max_iter){
      tag <- 0
      break
    }
  }
  return(list(x=x,error=error,iter_time=k,tag=tag))
}
```

## Gauss-Seidel.cpp

In addition, StatComp22074 also used Rcpp to complete the repetition of Gauss-Seidel iteration method for simulation comparison. The process of Gauss-Seidel iteration was described in **the Smoother** section above.

```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector Gauss_Seidel(NumericMatrix A,NumericVector b,NumericVector x0,int N=10) {
  int n = A.ncol();
  NumericVector x_old = x0 - 10;
  NumericVector x_new = x0;
  for(int i=1;i<N;i++){
    x_old = x_new;
    for(int j=0; j<n; j++)
    {
      double sum=0;
      for(int k=0; k<n; k++)
      {
        if(j>k)
        {
          sum+=(-1)*A(j,k)*x_new(k);
        }
        else if(j<k)
        {
          sum+=(-1)*A(j,k)*x_old(k);
        }
      }
      x_new[j]=(sum+b(j))/A(j,j);
    }
  }
  return x_new;
}
```


## Simulation

You can simply generate the data and simulate it in the following ways.

```{r,eval=TRUE}
library(Rcpp)
library(StatComp22074)

data("matrix_data_1e3")
attach(data)
set.seed(1)
n <- dim(A)[1]
x0 <- array(rep(0,n))

before <- Multigrid_Setup(A,layer = 10,CoarsenType='Projection',
                          InterpType='Projection',pri = TRUE)
start_time1 <- Sys.time()
x_notPre <- Multigrid_Solve(Ah=before$Ah,b,Qh=before$Qh,x0=x0,
                            presmooth='Gauss-Seidel',v1=5,postsmooth='Gauss-Seidel',v2=5,
                            solver='exact',tol=1e-6,max_iter=1e3,Precond=FALSE)
end_time1 <- Sys.time()
start_time2 <- Sys.time()
x_Pre <- Multigrid_Solve(Ah=before$Ah,b,Qh=before$Qh,x0=x0,
                         presmooth='Gauss-Seidel',v1=5,postsmooth='Gauss-Seidel',v2=5,
                         solver='exact',tol=1e-6,max_iter=1e3,Precond=TRUE)
end_time2 <- Sys.time()

N <- 5e2
start_time3 <- Sys.time()
x_R <- Smoother(A,b,x0,tol=1e-10,max_iter=N,SmoothMethod = 'Gauss-Seidel')
end_time3 <- Sys.time()

start_time4 <- Sys.time()
x_Cpp <- Gauss_Seidel(A,b,x0,N)
end_time4 <- Sys.time()

data_Count <- data.frame(method=c('AMG','Precondition AMG','Gauss Seidel_R','Gauss Seidel_Rcpp'),
                         time=c(end_time1-start_time1,end_time2-start_time2,end_time3-start_time3,
                                end_time4-start_time4),
                         iter_time = c(x_notPre$iter_time,x_Pre$iter_time,x_R$iter_time,N),
                         error=c(x_notPre$error,x_Pre$error,x_R$error,norm(b-A%*%x_Cpp)/norm(b)))

knitr::kable(data_Count)
```

The results show that the convergence rate of the multi-grid method is faster. At the same time, c++ runs faster and converges better.